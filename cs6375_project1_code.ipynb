{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning Project 1 - Benjamin Walmer"
      ],
      "metadata": {
        "id": "hZPJRuiDNbi2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A1UX4c0uS683"
      },
      "outputs": [],
      "source": [
        "# Using numpy and pandas\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4LiKL8XBjHL"
      },
      "source": [
        "Reading in training datasets from public github link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7CH1Kc85czxu"
      },
      "outputs": [],
      "source": [
        "# Importing libraries for reading files from github\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# Downloading the full repository (zipped)\n",
        "url = \"https://github.com/benwalmer/Email_Classification/archive/refs/heads/main.zip\"\n",
        "zip_path = \"repo.zip\"\n",
        "r = requests.get(url)\n",
        "with open(zip_path, \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "# Unnzipping the repository\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "repo_folder = [d for d in os.listdir() if d.endswith(\"-main\")][0]\n",
        "\n",
        "# Step 3: Iterating over all zipped datasets inside repo\n",
        "data = []\n",
        "for item in os.listdir(repo_folder):\n",
        "    if item.endswith(\"train.zip\"):\n",
        "        inner_zip_path = os.path.join(repo_folder, item)\n",
        "\n",
        "        # Extracting the dataset\n",
        "        with zipfile.ZipFile(inner_zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\".\")\n",
        "\n",
        "        dataset_name = item.replace(\"_train.zip\", \"\")\n",
        "        base_dir = \"\"\n",
        "        # Special case for enron2 because its file path is stored differently\n",
        "        if (dataset_name == \"enron2\"):\n",
        "            base_dir = \"train\"\n",
        "        else:\n",
        "            base_dir = os.path.join(dataset_name, \"train\")\n",
        "\n",
        "        # Step 4: Loading in spam/ham emails\n",
        "        for label in [\"spam\", \"ham\"]:\n",
        "            folder_path = os.path.join(base_dir, label)\n",
        "            if os.path.exists(folder_path):\n",
        "                for filename in os.listdir(folder_path):\n",
        "                    file_path = os.path.join(folder_path, filename)\n",
        "                    if os.path.isfile(file_path):\n",
        "                        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                            text = f.read()\n",
        "                        data.append((text, label, dataset_name))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BT_0X7fBriT"
      },
      "source": [
        "Splitting dataset into different training sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UkgxM6S6bGcJ"
      },
      "outputs": [],
      "source": [
        "# Data frame of spam/ham emails, split into different training sets\n",
        "df = pd.DataFrame(data, columns=[\"text\", \"label\", \"dataset\"])\n",
        "#make enron1 a new dataset and reset the indices\n",
        "enron1 = df[df['dataset'] == 'enron1']\n",
        "enron1 = enron1.reset_index(drop=True)\n",
        "enron2 = df[df['dataset'] == 'enron2']\n",
        "enron2 = enron2.reset_index(drop=True)\n",
        "enron4 = df[df['dataset'] == 'enron4']\n",
        "enron4 = enron4.reset_index(drop=True)\n",
        "enron1 = enron1.drop(columns=['dataset'])\n",
        "enron2 = enron2.drop(columns=['dataset'])\n",
        "enron4 = enron4.drop(columns=['dataset'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQIWdmb4Ti1G"
      },
      "source": [
        "Reading in test datasets from public github link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t2IpmkoDTamH"
      },
      "outputs": [],
      "source": [
        "# Step 3: Iterating over all zipped datasets inside repo\n",
        "test_data = []\n",
        "for item in os.listdir(repo_folder):\n",
        "    if item.endswith(\"test.zip\"):\n",
        "        inner_zip_path = os.path.join(repo_folder, item)\n",
        "\n",
        "        # Extracting the dataset\n",
        "        with zipfile.ZipFile(inner_zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\".\")\n",
        "\n",
        "        dataset_name = item.replace(\"_test.zip\", \"\")\n",
        "        base_dir = \"\"\n",
        "        # Special case for enron2 because its file path is stored differently\n",
        "        if (dataset_name == \"enron2\"):\n",
        "            base_dir = \"test\"\n",
        "        else:\n",
        "            base_dir = os.path.join(dataset_name, \"test\")\n",
        "        # Step 4: Loading in spam/ham emails\n",
        "        for label in [\"spam\", \"ham\"]:\n",
        "            folder_path = os.path.join(base_dir, label)\n",
        "            if os.path.exists(folder_path):\n",
        "                for filename in os.listdir(folder_path):\n",
        "                    file_path = os.path.join(folder_path, filename)\n",
        "                    if os.path.isfile(file_path):\n",
        "                        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                            text = f.read()\n",
        "                        test_data.append((text, label, dataset_name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXci1PAQYXdC"
      },
      "source": [
        "Splitting dataset into different test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GkhxfJWBUkle"
      },
      "outputs": [],
      "source": [
        "# Data frame of spam/ham emails, split into different training sets\n",
        "df_test = pd.DataFrame(test_data, columns=[\"text\", \"label\", \"dataset\"])\n",
        "#make enron1 a new dataset and reset the indices\n",
        "enron1_test = df_test[df_test['dataset'] == 'enron1']\n",
        "enron1_test = enron1_test.reset_index(drop=True)\n",
        "enron2_test = df_test[df_test['dataset'] == 'enron2']\n",
        "enron2_test = enron2_test.reset_index(drop=True)\n",
        "enron4_test = df_test[df_test['dataset'] == 'enron4']\n",
        "enron4_test = enron4_test.reset_index(drop=True)\n",
        "enron1_test = enron1_test.drop(columns=['dataset'])\n",
        "enron2_test = enron2_test.drop(columns=['dataset'])\n",
        "enron4_test = enron4_test.drop(columns=['dataset'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3vU6i6bYe7M"
      },
      "source": [
        "importing NLTK Processing Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7ZTYfXPumxU",
        "outputId": "b1b578d6-9fe2-4214-f78a-0b53e32a8905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Download NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b4UHxYUBw37"
      },
      "source": [
        "Training Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WbGuvHqnl7Qb"
      },
      "outputs": [],
      "source": [
        "# Preprocessing Emails\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Step 1: Preprocess text\n",
        "def preprocess(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove all text between \"forwarded\" and \"subject\", multiple times if present\n",
        "    text = re.sub(r'forwarded.*?subject', '', text, flags=re.DOTALL)\n",
        "\n",
        "    text = re.sub(r'original.*?subject', '', text, flags=re.DOTALL)\n",
        "\n",
        "    text = re.sub(r'\\b(from|to|cc|subject|re)\\b', '', text)\n",
        "\n",
        "    # Remove newline characters\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Clean tokens\n",
        "    clean_tokens = []\n",
        "    for t in tokens:\n",
        "        # Remove punctuation, numbers, and special characters from each token\n",
        "        t = re.sub(r'[^a-z]', '', t)\n",
        "        if t and t not in stop_words:\n",
        "            clean_tokens.append(t)\n",
        "    return clean_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FJ9-yyl2suOJ"
      },
      "outputs": [],
      "source": [
        "def buildvocab(df):\n",
        "  processed_texts = df['text'].apply(preprocess)\n",
        "  all_words = Counter()\n",
        "  for tokens in processed_texts:\n",
        "      all_words.update(tokens)\n",
        "  all_words = sorted(all_words)\n",
        "  return processed_texts, all_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XpejqYpkpzGW"
      },
      "outputs": [],
      "source": [
        "def generate_df(processed_texts, vocab, labels):\n",
        "  bow_mat = []\n",
        "  for tokens in processed_texts:\n",
        "    word_counts = Counter(tokens)\n",
        "    row = [word_counts.get(word, 0) for word in vocab]\n",
        "    bow_mat.append(row)\n",
        "  vocab_bow = pd.DataFrame(bow_mat, columns=vocab)\n",
        "  vocab_bow['label'] = labels\n",
        "  ber_mat = []\n",
        "  for tokens in processed_texts:\n",
        "    token_set = set(tokens)\n",
        "    row = [1 if word in token_set else 0 for word in vocab]\n",
        "    ber_mat.append(row)\n",
        "  vocab_ber = pd.DataFrame(ber_mat, columns=vocab)\n",
        "  vocab_ber['label'] = labels\n",
        "  return vocab_bow, vocab_ber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AIFw51meqwC8"
      },
      "outputs": [],
      "source": [
        "# Training Data\n",
        "p1, e1_vocab = buildvocab(enron1)\n",
        "enron1_bow_train, enron1_bernoulli_train = generate_df(p1, e1_vocab, enron1['label'])\n",
        "p2, e2_vocab = buildvocab(enron2)\n",
        "enron2_bow_train, enron2_bernoulli_train = generate_df(p2, e2_vocab, enron2['label'])\n",
        "p4, e4_vocab = buildvocab(enron4)\n",
        "enron4_bow_train, enron4_bernoulli_train = generate_df(p4, e4_vocab, enron4['label'])\n",
        "\n",
        "# Test Data\n",
        "enron1_bow_test, enron1_bernoulli_test = generate_df(enron1_test['text'].apply(preprocess), e1_vocab, enron1_test['label'])\n",
        "enron2_bow_test, enron2_bernoulli_test = generate_df(enron2_test['text'].apply(preprocess), e2_vocab, enron2_test['label'])\n",
        "enron4_bow_test, enron4_bernoulli_test = generate_df(enron4_test['text'].apply(preprocess), e4_vocab, enron4_test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r2vvMja5rm8k"
      },
      "outputs": [],
      "source": [
        "# Exporting all training files\n",
        "enron1_bow_train.to_csv('enron1_bow_train.csv', index=False)\n",
        "enron1_bernoulli_train.to_csv('enron1_bernoulli_train.csv', index=False)\n",
        "enron2_bow_train.to_csv('enron2_bow_train.csv', index=False)\n",
        "enron2_bernoulli_train.to_csv('enron2_bernoulli_train.csv', index=False)\n",
        "enron4_bow_train.to_csv('enron4_bow_train.csv', index=False)\n",
        "enron4_bernoulli_train.to_csv('enron4_bernoulli_train.csv', index=False)\n",
        "\n",
        "# Exporting all test files\n",
        "enron1_bow_test.to_csv('enron1_bow_test.csv', index=False)\n",
        "enron1_bernoulli_test.to_csv('enron1_bernoulli_test.csv', index=False)\n",
        "enron2_bow_test.to_csv('enron2_bow_test.csv', index=False)\n",
        "enron2_bernoulli_test.to_csv('enron2_bernoulli_test.csv', index=False)\n",
        "enron4_bow_test.to_csv('enron4_bow_test.csv', index=False)\n",
        "enron4_bernoulli_test.to_csv('enron4_bernoulli_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary Statistics for Each Dataset:"
      ],
      "metadata": {
        "id": "3CFZx-sDNUVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOW:"
      ],
      "metadata": {
        "id": "chpgsBd-NZa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"enron1_train Summary Statistics:\")\n",
        "print(\"Number of emails:\", enron1_bow_train.shape[0])\n",
        "print(\"Number of spam emails:\", enron1_bow_train[enron1_bow_train['label'] == 'spam'].shape[0])\n",
        "print(\"Number of ham emails:\", enron1_bow_train[enron1_bow_train['label'] == 'ham'].shape[0])\n",
        "print(\"Number of unique vocabulary words:\", enron1_bow_train.shape[1]-1)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"enron1_test Summary Statistics:\")\n",
        "print(\"Number of emails:\", enron1_bow_test.shape[0])\n",
        "print(\"Number of spam emails:\", enron1_bow_test[enron1_bow_test['label'] == 'spam'].shape[0])\n",
        "print(\"Number of ham emails:\", enron1_bow_test[enron1_bow_test['label'] == 'ham'].shape[0])\n",
        "print(\"Number of unique vocabulary words:\", enron1_bow_test.shape[1]-1)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"enron2_train Summary Statistics:\")\n",
        "print(\"Number of emails:\", enron2_bow_train.shape[0])\n",
        "print(\"Number of spam emails:\", enron2_bow_train[enron2_bow_train['label'] == 'spam'].shape[0])\n",
        "print(\"Number of ham emails:\", enron2_bow_train[enron2_bow_train['label'] == 'ham'].shape[0])\n",
        "print(\"Number of unique vocabulary words:\", enron2_bow_train.shape[1]-1)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"enron2_test Summary Statistics:\")\n",
        "print(\"Number of emails:\", enron2_bow_test.shape[0])\n",
        "print(\"Number of spam emails:\", enron2_bow_test[enron2_bow_test['label'] == 'spam'].shape[0])\n",
        "print(\"Number of ham emails:\", enron2_bow_test[enron2_bow_test['label'] == 'ham'].shape[0])\n",
        "print(\"Number of unique vocabulary words:\", enron2_bow_test.shape[1]-1)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"enron4_train Summary Statistics:\")\n",
        "print(\"Number of emails:\", enron4_bow_train.shape[0])\n",
        "print(\"Number of spam emails:\", enron4_bow_train[enron4_bow_train['label'] == 'spam'].shape[0])\n",
        "print(\"Number of ham emails:\", enron4_bow_train[enron4_bow_train['label'] == 'ham'].shape[0])\n",
        "print(\"Number of unique vocabulary words:\", enron4_bow_train.shape[1]-1)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"enron4_test Summary Statistics:\")\n",
        "print(\"Number of emails:\", enron4_bow_test.shape[0])\n",
        "print(\"Number of spam emails:\", enron4_bow_test[enron4_bow_test['label'] == 'spam'].shape[0])\n",
        "print(\"Number of ham emails:\", enron4_bow_test[enron4_bow_test['label'] == 'ham'].shape[0])\n",
        "print(\"Number of unique vocabulary words:\", enron4_bow_test.shape[1]-1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyaeqGKMNY8-",
        "outputId": "6bce9011-3758-4f89-b44e-90e467fea42a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enron1_train Summary Statistics:\n",
            "Number of emails: 450\n",
            "Number of spam emails: 131\n",
            "Number of ham emails: 319\n",
            "Number of unique vocabulary words: 8238\n",
            "\n",
            "\n",
            "enron1_test Summary Statistics:\n",
            "Number of emails: 456\n",
            "Number of spam emails: 149\n",
            "Number of ham emails: 307\n",
            "Number of unique vocabulary words: 8238\n",
            "\n",
            "\n",
            "enron2_train Summary Statistics:\n",
            "Number of emails: 463\n",
            "Number of spam emails: 123\n",
            "Number of ham emails: 340\n",
            "Number of unique vocabulary words: 8818\n",
            "\n",
            "\n",
            "enron2_test Summary Statistics:\n",
            "Number of emails: 478\n",
            "Number of spam emails: 130\n",
            "Number of ham emails: 348\n",
            "Number of unique vocabulary words: 8818\n",
            "\n",
            "\n",
            "enron4_train Summary Statistics:\n",
            "Number of emails: 535\n",
            "Number of spam emails: 402\n",
            "Number of ham emails: 133\n",
            "Number of unique vocabulary words: 16322\n",
            "\n",
            "\n",
            "enron4_test Summary Statistics:\n",
            "Number of emails: 543\n",
            "Number of spam emails: 391\n",
            "Number of ham emails: 152\n",
            "Number of unique vocabulary words: 16322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6TWzTk8Ov5I"
      },
      "source": [
        "Modeling..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9evPrX7Oxo_"
      },
      "source": [
        "Naive Bayes Multinomial (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DMgCa1IWuc6W"
      },
      "outputs": [],
      "source": [
        "def mnb(vocab, df_original, df):\n",
        "  V = vocab\n",
        "  V_len = len(vocab)\n",
        "  N = len(df_original)\n",
        "  spam = df[df['label'] == 'spam']\n",
        "  ham = df[df['label'] == 'ham']\n",
        "  prior_spam = len(spam) / N\n",
        "  prior_ham = len(ham) / N\n",
        "\n",
        "  text_spam = spam.drop(columns=['label']).sum()\n",
        "  text_ham = ham.drop(columns=['label']).sum()\n",
        "\n",
        "  denom_spam = text_spam.sum() + V_len\n",
        "  denom_ham = text_ham.sum() + V_len\n",
        "\n",
        "  text_spam = pd.DataFrame({\n",
        "    'count': text_spam,\n",
        "    'conditional': (text_spam + 1) / denom_spam\n",
        "})\n",
        "\n",
        "  text_ham = pd.DataFrame({\n",
        "    'count': text_ham,\n",
        "    'conditional': (text_ham + 1) / denom_ham\n",
        "})\n",
        "  return prior_spam, prior_ham, text_spam, text_ham, V\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OYXj-BFwxDOr"
      },
      "outputs": [],
      "source": [
        "def predict_mnb (prior_spam, prior_ham, text_spam, text_ham, V, tokens):\n",
        "  import math\n",
        "  spam_likelihood = math.log(prior_spam)\n",
        "  ham_likelihood = math.log(prior_ham)\n",
        "  for token in tokens:\n",
        "      if token in V:\n",
        "          spam_likelihood += math.log(text_spam.loc[token, \"conditional\"])\n",
        "          ham_likelihood += math.log(text_ham.loc[token, \"conditional\"])\n",
        "\n",
        "  return 'spam' if spam_likelihood >= ham_likelihood else 'ham'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Fh_eO_6-zcBd"
      },
      "outputs": [],
      "source": [
        "def evaluate(df):\n",
        "  total = len(df)\n",
        "  spam_correct = sum((df['label'] == 'spam') & (df['pred_label'] == 'spam'))\n",
        "  spam_incorrect = sum((df['label'] == 'ham') & (df['pred_label'] == 'spam'))\n",
        "  ham_incorrect = sum((df['label'] == 'spam') & (df['pred_label'] == 'ham'))\n",
        "  ham_correct = sum((df['label'] == 'ham') & (df['pred_label'] == 'ham'))\n",
        "\n",
        "  accuracy = ((spam_correct + ham_correct) / total)\n",
        "  precision = (spam_correct / (spam_correct + spam_incorrect))\n",
        "  recall = (spam_correct / (spam_correct + ham_incorrect))\n",
        "  f1_score = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "  print(f\"Accuracy: {accuracy:.4f}\")\n",
        "  print(f\"Precision: {precision: .4f}\")\n",
        "  print(f\"Recall: {recall: .4f}\")\n",
        "  print(f\"F1 Score: {f1_score: .4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE4oHY-yzsnJ"
      },
      "source": [
        "Enron1 MNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtKLuVrNv98s",
        "outputId": "13e36ebe-04ff-4d1d-8aba-f2c2b9f965ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 1 MNB Evaluation Metrics:\n",
            "Accuracy: 0.9298\n",
            "Precision:  0.9333\n",
            "Recall:  0.8456\n",
            "F1 Score:  0.8873\n"
          ]
        }
      ],
      "source": [
        "enron1_bow_test_mnb = enron1_bow_test.copy()\n",
        "ps1, ph1, ts1, th1, v1 = mnb(e1_vocab, enron1, enron1_bow_train)\n",
        "enron1_bow_test_mnb['pred_label'] = enron1_test['text'].apply(preprocess).apply(lambda tokens: predict_mnb(ps1, ph1, ts1, th1, v1, tokens))\n",
        "print (\"Enron 1 MNB Evaluation Metrics:\")\n",
        "evaluate(enron1_bow_test_mnb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OAEoQoAe-KV"
      },
      "source": [
        "Enron2 MNB\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z834A1eHz1A5",
        "outputId": "47e8afdd-0336-4e12-caeb-208138549d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 2 MNB Evaluation Metrics:\n",
            "Accuracy: 0.9435\n",
            "Precision:  0.9256\n",
            "Recall:  0.8615\n",
            "F1 Score:  0.8924\n"
          ]
        }
      ],
      "source": [
        "enron2_bow_test_mnb = enron2_bow_test.copy()\n",
        "ps2, ph2, ts2, th2, v2 = mnb(e2_vocab, enron2, enron2_bow_train)\n",
        "enron2_bow_test_mnb['pred_label'] = enron2_test['text'].apply(preprocess).apply(lambda tokens: predict_mnb(ps2, ph2, ts2, th2, v2, tokens))\n",
        "print (\"Enron 2 MNB Evaluation Metrics:\")\n",
        "evaluate(enron2_bow_test_mnb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syvCO_OrgdLW"
      },
      "source": [
        "Enron4 MNB\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IpMLFa60A_6",
        "outputId": "c849b94d-9700-454c-d4d2-a2e6521424c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 4 MNB Evaluation Metrics:\n",
            "Accuracy: 0.9540\n",
            "Precision:  0.9598\n",
            "Recall:  0.9770\n",
            "F1 Score:  0.9683\n"
          ]
        }
      ],
      "source": [
        "enron4_bow_test_mnb = enron4_bow_test.copy()\n",
        "ps4, ph4, ts4, th4, v4 = mnb(e4_vocab, enron4, enron4_bow_train)\n",
        "enron4_bow_test_mnb['pred_label'] = enron4_test['text'].apply(preprocess).apply(lambda tokens: predict_mnb(ps4, ph4, ts4, th4, v4, tokens))\n",
        "print (\"Enron 4 MNB Evaluation Metrics:\")\n",
        "evaluate(enron4_bow_test_mnb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbgySmebjTal"
      },
      "source": [
        "Enron1 Bernoulli Naive Bayes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "avAxkl5s0ot9"
      },
      "outputs": [],
      "source": [
        "def bnb(vocab, df_original, df):\n",
        "  V = vocab\n",
        "\n",
        "  e1ber_spam = df[df['label'] == 'spam']\n",
        "  e1ber_ham = df[df['label'] == 'ham']\n",
        "\n",
        "  textber_spam = e1ber_spam.drop(columns=['label']).sum()\n",
        "  textber_ham = e1ber_ham.drop(columns=['label']).sum()\n",
        "\n",
        "  spams = e1ber_spam.shape[0]\n",
        "  hams = e1ber_ham.shape[0]\n",
        "  prior_spamber = spams / (spams + hams)\n",
        "  prior_hamber = hams / (spams + hams)\n",
        "\n",
        "  # Add conditionals as new columns (Laplace smoothing)\n",
        "  textber_spam = pd.DataFrame({\n",
        "      'count': textber_spam,\n",
        "      'conditional': ((textber_spam + 1) / (len(e1ber_spam) + 2))\n",
        "  })\n",
        "\n",
        "  textber_ham = pd.DataFrame({\n",
        "      'count': textber_ham,\n",
        "      'conditional': ((textber_ham + 1) / (len(e1ber_ham) + 2))\n",
        "  })\n",
        "\n",
        "  return prior_spamber, prior_hamber, textber_spam, textber_ham, V\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "J3zpIRkH1Vkb"
      },
      "outputs": [],
      "source": [
        "def predict_bnb(prior_spamber, prior_hamber, textber_spam, textber_ham, V, tokens):\n",
        "  import math\n",
        "  spam_likelihood = math.log(prior_spamber)\n",
        "  ham_likelihood = math.log(prior_hamber)\n",
        "\n",
        "  for token in V:\n",
        "      if token in tokens:\n",
        "        spam_likelihood += math.log(textber_spam.loc[token, \"conditional\"])\n",
        "        ham_likelihood += math.log(textber_ham.loc[token, \"conditional\"])\n",
        "      else:\n",
        "        spam_likelihood += math.log(1 - textber_spam.loc[token, \"conditional\"])\n",
        "        ham_likelihood += math.log(1 - textber_ham.loc[token, \"conditional\"])\n",
        "\n",
        "  return 'spam' if spam_likelihood >= ham_likelihood else 'ham'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmOnG19Y1rKQ",
        "outputId": "9d318e31-376c-49d6-dfc4-a25998d9d0ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 1 Bernoulli NB Evaluation Metrics:\n",
            "Accuracy: 0.7215\n",
            "Precision:  0.8438\n",
            "Recall:  0.1812\n",
            "F1 Score:  0.2983\n"
          ]
        }
      ],
      "source": [
        "enron1_bernoulli_test_bnb = enron1_bernoulli_test.copy()\n",
        "ps11, ph11, ts11, th11, v11 = bnb(e1_vocab, enron1, enron1_bernoulli_train)\n",
        "enron1_bernoulli_test_bnb['pred_label'] = enron1_test['text'].apply(preprocess).apply(lambda tokens: predict_bnb(ps11, ph11, ts11, th11, v11, tokens))\n",
        "print(\"Enron 1 Bernoulli NB Evaluation Metrics:\")\n",
        "evaluate(enron1_bernoulli_test_bnb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D893IIu2a9z",
        "outputId": "c840030f-fce5-4539-9a85-2d5bc2f1bd34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 2 Bernoulli NB Evaluation Metrics:\n",
            "Accuracy: 0.7762\n",
            "Precision:  0.8966\n",
            "Recall:  0.2000\n",
            "F1 Score:  0.3270\n"
          ]
        }
      ],
      "source": [
        "enron2_bernoulli_test_bnb = enron2_bernoulli_test.copy()\n",
        "ps22, ph22, ts22, th22, v22 = bnb(e2_vocab, enron2, enron2_bernoulli_train)\n",
        "enron2_bernoulli_test_bnb['pred_label'] = enron2_test['text'].apply(preprocess).apply(lambda tokens: predict_bnb(ps22, ph22, ts22, th22, v22, tokens))\n",
        "print(\"Enron 2 Bernoulli NB Evaluation Metrics:\")\n",
        "evaluate(enron2_bernoulli_test_bnb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9anY_gp92d61",
        "outputId": "6ed91161-ea64-4ba1-ad79-f5e58f115962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 4 Bernoulli NB Evaluation Metrics:\n",
            "Accuracy: 0.9098\n",
            "Precision:  0.8886\n",
            "Recall:  1.0000\n",
            "F1 Score:  0.9410\n"
          ]
        }
      ],
      "source": [
        "enron4_bernoulli_test_bnb = enron4_bernoulli_test.copy()\n",
        "ps44, ph44, ts44, th44, v44 = bnb(e4_vocab, enron4, enron4_bernoulli_train)\n",
        "enron4_bernoulli_test_bnb['pred_label'] = enron4_test['text'].apply(preprocess).apply(lambda tokens: predict_bnb(ps44, ph44, ts44, th44, v44, tokens))\n",
        "print(\"Enron 4 Bernoulli NB Evaluation Metrics:\")\n",
        "evaluate(enron4_bernoulli_test_bnb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5uVsIw5aNuX"
      },
      "source": [
        "Logistic Regression..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3i-MxosEaPQs"
      },
      "outputs": [],
      "source": [
        "def log_regression(X, lr, T, ld):\n",
        "    # Extract y and map labels to {0,1}\n",
        "    y = X[:, -1].copy()\n",
        "    y = np.where(y == 'ham', 0, 1).astype(float)\n",
        "\n",
        "    # Features (remove label column)\n",
        "    X = X[:, :-1].astype(float)\n",
        "    d, n = X.shape\n",
        "\n",
        "    # Add bias column\n",
        "    X = np.hstack((np.ones((d, 1)), X))\n",
        "\n",
        "    # Initialize weights\n",
        "    W = np.ones(n + 1)\n",
        "\n",
        "    for t in range(T):\n",
        "        # Linear combination (vectorized)\n",
        "        z = X @ W\n",
        "        z = np.clip(z, -500, 500)  # numerical stability\n",
        "\n",
        "        # Sigmoid\n",
        "        ypred = 1 / (1 + np.exp(-z))\n",
        "\n",
        "        # Gradient (vectorized)\n",
        "        g = X.T @ (y - ypred)\n",
        "        g[1:] -= ld * W[1:]  # L2 penalty (skip bias)\n",
        "\n",
        "        # Update weights\n",
        "        W += lr * g\n",
        "\n",
        "    return W\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(df):\n",
        "  total = len(df)\n",
        "  spam_correct = sum((df['label'] == 'spam') & (df['pred_label'] == 'spam'))\n",
        "  ham_correct = sum((df['label'] == 'ham') & (df['pred_label'] == 'ham'))\n",
        "  accuracy = ((spam_correct + ham_correct) / total)\n",
        "  return accuracy\n"
      ],
      "metadata": {
        "id": "Y6VGeBI4lTOi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SIRBRLfIceZr"
      },
      "outputs": [],
      "source": [
        "def tuning(df):\n",
        "    \"\"\"\n",
        "    df: pandas DataFrame (or numpy array) with features + label in last column.\n",
        "        If df is a DataFrame, convert to numpy before calling log_regression.\n",
        "    lr_list: list of learning rates to try\n",
        "    T: iterations for log_regression\n",
        "    ld: lambda (regularization) fixed while tuning lr\n",
        "    Returns best_lr (lowest validation log loss) and a dict mapping lr->val_loss\n",
        "    \"\"\"\n",
        "    T = [1000]\n",
        "    ld = [0, 2, 3, 4, 5, 6, 8, 10, 12, 14, 16, 18, 20]\n",
        "    lr_list = [0.001]\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    # Convert DataFrame to numpy array before splitting\n",
        "    df_np = df.to_numpy()\n",
        "    X_train, X_val = train_test_split(df_np, test_size=0.3, random_state=11)\n",
        "    results = {}\n",
        "\n",
        "    for t in T:\n",
        "        for l in ld:\n",
        "          for lr in lr_list:\n",
        "            W = log_regression(X_train.copy(), lr, t, l)\n",
        "            val = pd.DataFrame(X_val, columns=df.columns)\n",
        "            val['pred_label'] = predict(val, W)\n",
        "            val['pred_label'] = val['pred_label'].apply(lambda x: 'spam' if x > 0.5 else 'ham')\n",
        "            val_acc = accuracy(val)\n",
        "            results[(lr, t, l)] = val_acc\n",
        "            print(f\"t: {t}  ld: {l}  lr: {lr:.4f} validation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    best_params = max(results, key=results.get)\n",
        "    print(\"Best iterations:\" + str(best_params[1]))\n",
        "    print(\"Best learning rate:\" + str(best_params[0]))\n",
        "    print(\"Best lambda:\" + str(best_params[2]))\n",
        "    return best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xesaiN_aQxP"
      },
      "source": [
        "Enron1 BOW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "uScWdjbiEqbH"
      },
      "outputs": [],
      "source": [
        "def predict(X, W):\n",
        "    X_test = X.to_numpy()\n",
        "    X_test = X_test[:, :-1].astype(float)\n",
        "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
        "    z = np.clip(X_test @ W, -500, 500)\n",
        "    return 1 / (1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDkK4xWRdCRb",
        "outputId": "9abb69ac-7fbb-4b71-f3c5-4a5ff62154f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 1 Bag of Words Parameter Tuning:\n",
            "t: 1000  ld: 0  lr: 0.0010 validation accuracy: 0.8815\n",
            "t: 1000  ld: 2  lr: 0.0010 validation accuracy: 0.9704\n",
            "t: 1000  ld: 3  lr: 0.0010 validation accuracy: 0.9630\n",
            "t: 1000  ld: 4  lr: 0.0010 validation accuracy: 0.9704\n",
            "t: 1000  ld: 5  lr: 0.0010 validation accuracy: 0.9556\n",
            "t: 1000  ld: 6  lr: 0.0010 validation accuracy: 0.9778\n",
            "t: 1000  ld: 8  lr: 0.0010 validation accuracy: 0.9704\n",
            "t: 1000  ld: 10  lr: 0.0010 validation accuracy: 0.9630\n",
            "t: 1000  ld: 12  lr: 0.0010 validation accuracy: 0.9556\n",
            "t: 1000  ld: 14  lr: 0.0010 validation accuracy: 0.9407\n",
            "t: 1000  ld: 16  lr: 0.0010 validation accuracy: 0.9481\n",
            "t: 1000  ld: 18  lr: 0.0010 validation accuracy: 0.9481\n",
            "t: 1000  ld: 20  lr: 0.0010 validation accuracy: 0.9407\n",
            "Best iterations:1000\n",
            "Best learning rate:0.001\n",
            "Best lambda:6\n"
          ]
        }
      ],
      "source": [
        "print(\"Enron 1 Bag of Words Parameter Tuning:\")\n",
        "e1_bow_params = tuning(enron1_bow_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "izfERD0FLTtW"
      },
      "outputs": [],
      "source": [
        "W_e1_bow = log_regression(enron1_bow_train.to_numpy(), e1_bow_params[0], e1_bow_params[1], e1_bow_params[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKo43wvo-e4W",
        "outputId": "cf58f79e-1d32-47f5-ed6e-2add601d8e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 1 BOW Logistic Regression Evaluation Metrics:\n",
            "Accuracy: 0.9320\n",
            "Precision:  0.9155\n",
            "Recall:  0.8725\n",
            "F1 Score:  0.8935\n"
          ]
        }
      ],
      "source": [
        "enron1_bow_test_lr = enron1_bow_test.copy()\n",
        "enron1_bow_test_lr['pred_label'] = predict(enron1_bow_test_lr, W_e1_bow)\n",
        "enron1_bow_test_lr['pred_label'] = enron1_bow_test_lr['pred_label'].apply(lambda x: 'spam' if x > 0.5 else 'ham')\n",
        "print(\"Enron 1 BOW Logistic Regression Evaluation Metrics:\")\n",
        "evaluate(enron1_bow_test_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU7I9M7VJW69",
        "outputId": "44ab5af4-276b-4f19-da4d-c7e612131e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 1 Bernoulli Parameter Tuning\n",
            "t: 1000  ld: 0  lr: 0.0010 validation accuracy: 0.9185\n",
            "t: 1000  ld: 2  lr: 0.0010 validation accuracy: 0.9407\n",
            "t: 1000  ld: 3  lr: 0.0010 validation accuracy: 0.9481\n",
            "t: 1000  ld: 4  lr: 0.0010 validation accuracy: 0.9333\n",
            "t: 1000  ld: 5  lr: 0.0010 validation accuracy: 0.9259\n",
            "t: 1000  ld: 6  lr: 0.0010 validation accuracy: 0.9259\n",
            "t: 1000  ld: 8  lr: 0.0010 validation accuracy: 0.9111\n",
            "t: 1000  ld: 10  lr: 0.0010 validation accuracy: 0.9111\n",
            "t: 1000  ld: 12  lr: 0.0010 validation accuracy: 0.9037\n",
            "t: 1000  ld: 14  lr: 0.0010 validation accuracy: 0.8889\n",
            "t: 1000  ld: 16  lr: 0.0010 validation accuracy: 0.8889\n",
            "t: 1000  ld: 18  lr: 0.0010 validation accuracy: 0.8815\n",
            "t: 1000  ld: 20  lr: 0.0010 validation accuracy: 0.8815\n",
            "Best iterations:1000\n",
            "Best learning rate:0.001\n",
            "Best lambda:3\n"
          ]
        }
      ],
      "source": [
        "print(\"Enron 1 Bernoulli Parameter Tuning\")\n",
        "e1_bernoulli_params = tuning(enron1_bernoulli_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "4f4NizFLMEYE"
      },
      "outputs": [],
      "source": [
        "W_e1_bernoulli = log_regression(enron1_bernoulli_train.to_numpy(), e1_bernoulli_params[0], e1_bernoulli_params[1], e1_bernoulli_params[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdcB--QMKAce",
        "outputId": "be78b7ef-8120-480b-a33b-c9fa8596b619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 1 Bernoulli Logistic Regression Evaluation Metrics:\n",
            "Accuracy: 0.8947\n",
            "Precision:  0.9469\n",
            "Recall:  0.7181\n",
            "F1 Score:  0.8168\n"
          ]
        }
      ],
      "source": [
        "enron1_bernoulli_test_lr = enron1_bernoulli_test.copy()\n",
        "enron1_bernoulli_test_lr['pred_label'] = predict(enron1_bernoulli_test_lr, W_e1_bernoulli)\n",
        "enron1_bernoulli_test_lr['pred_label'] = enron1_bernoulli_test_lr['pred_label'].apply(lambda x: 'spam' if x > 0.5 else 'ham')\n",
        "print(\"Enron 1 Bernoulli Logistic Regression Evaluation Metrics:\")\n",
        "evaluate(enron1_bernoulli_test_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIqHaSQFKpGE",
        "outputId": "591fa15f-8c79-4975-96dc-03224b0fe447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 2 Bag of Words Parameter Tuning:\n",
            "t: 1000  ld: 0  lr: 0.0010 validation accuracy: 0.8058\n",
            "t: 1000  ld: 2  lr: 0.0010 validation accuracy: 0.8705\n",
            "t: 1000  ld: 3  lr: 0.0010 validation accuracy: 0.8921\n",
            "t: 1000  ld: 4  lr: 0.0010 validation accuracy: 0.8993\n",
            "t: 1000  ld: 5  lr: 0.0010 validation accuracy: 0.9065\n",
            "t: 1000  ld: 6  lr: 0.0010 validation accuracy: 0.9065\n",
            "t: 1000  ld: 8  lr: 0.0010 validation accuracy: 0.9137\n",
            "t: 1000  ld: 10  lr: 0.0010 validation accuracy: 0.9137\n",
            "t: 1000  ld: 12  lr: 0.0010 validation accuracy: 0.9137\n",
            "t: 1000  ld: 14  lr: 0.0010 validation accuracy: 0.9137\n",
            "t: 1000  ld: 16  lr: 0.0010 validation accuracy: 0.9137\n",
            "t: 1000  ld: 18  lr: 0.0010 validation accuracy: 0.9065\n",
            "t: 1000  ld: 20  lr: 0.0010 validation accuracy: 0.9065\n",
            "Best iterations:1000\n",
            "Best learning rate:0.001\n",
            "Best lambda:8\n"
          ]
        }
      ],
      "source": [
        "print(\"Enron 2 Bag of Words Parameter Tuning:\")\n",
        "e2_bow_params = tuning(enron2_bow_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "7DwWQZFpNnzV"
      },
      "outputs": [],
      "source": [
        "W_e2_bow = log_regression(enron2_bow_train.to_numpy(), e2_bow_params[0], e2_bow_params[1], e2_bow_params[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhtpLTCh-4X2",
        "outputId": "194f1d63-fcec-49c8-dfbd-775f62547c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 2 BOW Logistic Regression Evaluation Metrics:\n",
            "Accuracy: 0.8996\n",
            "Precision:  0.9184\n",
            "Recall:  0.6923\n",
            "F1 Score:  0.7895\n"
          ]
        }
      ],
      "source": [
        "enron2_bow_test_lr = enron2_bow_test.copy()\n",
        "enron2_bow_test_lr['pred_label'] = predict(enron2_bow_test_lr, W_e2_bow)\n",
        "enron2_bow_test_lr['pred_label'] = enron2_bow_test_lr['pred_label'].apply(lambda x: 'spam' if x > 0.5 else 'ham')\n",
        "print(\"Enron 2 BOW Logistic Regression Evaluation Metrics:\")\n",
        "evaluate(enron2_bow_test_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXn_2LgZK-vg",
        "outputId": "3dc0dc65-a179-4a63-ac31-b31bf5a56c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 2 Bernoulli Parameter Tuning:\n",
            "t: 1000  ld: 0  lr: 0.0010 validation accuracy: 0.8705\n",
            "t: 1000  ld: 2  lr: 0.0010 validation accuracy: 0.8705\n",
            "t: 1000  ld: 3  lr: 0.0010 validation accuracy: 0.8849\n",
            "t: 1000  ld: 4  lr: 0.0010 validation accuracy: 0.8993\n",
            "t: 1000  ld: 5  lr: 0.0010 validation accuracy: 0.9137\n",
            "t: 1000  ld: 6  lr: 0.0010 validation accuracy: 0.9137\n",
            "t: 1000  ld: 8  lr: 0.0010 validation accuracy: 0.9065\n",
            "t: 1000  ld: 10  lr: 0.0010 validation accuracy: 0.9065\n",
            "t: 1000  ld: 12  lr: 0.0010 validation accuracy: 0.9065\n",
            "t: 1000  ld: 14  lr: 0.0010 validation accuracy: 0.8993\n",
            "t: 1000  ld: 16  lr: 0.0010 validation accuracy: 0.8993\n",
            "t: 1000  ld: 18  lr: 0.0010 validation accuracy: 0.8993\n",
            "t: 1000  ld: 20  lr: 0.0010 validation accuracy: 0.8921\n",
            "Best iterations:1000\n",
            "Best learning rate:0.001\n",
            "Best lambda:5\n"
          ]
        }
      ],
      "source": [
        "print(\"Enron 2 Bernoulli Parameter Tuning:\")\n",
        "e2_bernoulli_params = tuning(enron2_bernoulli_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "EC5SHs-QN6H_"
      },
      "outputs": [],
      "source": [
        "W_e2_bernoulli = log_regression(enron2_bernoulli_train.to_numpy(), e2_bernoulli_params[0], e2_bernoulli_params[1], e2_bernoulli_params[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BdfM2W_LFNu",
        "outputId": "f434f800-62af-4239-9068-6a1d6c25151e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 2 Bernoulli Logistic Regression Evaluation Metrics:\n",
            "Accuracy: 0.8808\n",
            "Precision:  0.8842\n",
            "Recall:  0.6462\n",
            "F1 Score:  0.7467\n"
          ]
        }
      ],
      "source": [
        "enron2_bernoulli_test_lr = enron2_bernoulli_test.copy()\n",
        "enron2_bernoulli_test_lr['pred_label'] = predict(enron2_bernoulli_test_lr, W_e2_bernoulli)\n",
        "enron2_bernoulli_test_lr['pred_label'] = enron2_bernoulli_test_lr['pred_label'].apply(lambda x: 'spam' if x > 0.5 else 'ham')\n",
        "print(\"Enron 2 Bernoulli Logistic Regression Evaluation Metrics:\")\n",
        "evaluate(enron2_bernoulli_test_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrKv39yHLNmF",
        "outputId": "ba4cd828-7fd9-4a40-b892-96921dd7215b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 4 Bag of Words Parameter Tuning:\n",
            "t: 1000  ld: 0  lr: 0.0010 validation accuracy: 0.9068\n",
            "t: 1000  ld: 2  lr: 0.0010 validation accuracy: 0.9441\n",
            "t: 1000  ld: 3  lr: 0.0010 validation accuracy: 0.9565\n",
            "t: 1000  ld: 4  lr: 0.0010 validation accuracy: 0.9565\n",
            "t: 1000  ld: 5  lr: 0.0010 validation accuracy: 0.9565\n",
            "t: 1000  ld: 6  lr: 0.0010 validation accuracy: 0.9565\n",
            "t: 1000  ld: 8  lr: 0.0010 validation accuracy: 0.9565\n",
            "t: 1000  ld: 10  lr: 0.0010 validation accuracy: 0.9565\n",
            "t: 1000  ld: 12  lr: 0.0010 validation accuracy: 0.9565\n",
            "t: 1000  ld: 14  lr: 0.0010 validation accuracy: 0.9565\n",
            "t: 1000  ld: 16  lr: 0.0010 validation accuracy: 0.9565\n",
            "t: 1000  ld: 18  lr: 0.0010 validation accuracy: 0.9565\n",
            "t: 1000  ld: 20  lr: 0.0010 validation accuracy: 0.9565\n",
            "Best iterations:1000\n",
            "Best learning rate:0.001\n",
            "Best lambda:3\n"
          ]
        }
      ],
      "source": [
        "print(\"Enron 4 Bag of Words Parameter Tuning:\")\n",
        "e4_bow_params = tuning(enron4_bow_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W_e4_bow = log_regression(enron4_bow_train.to_numpy(), e4_bow_params[0], e4_bow_params[1], e4_bow_params[2])"
      ],
      "metadata": {
        "id": "nFd-bUJmhhjV"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsUp_WluLTDy",
        "outputId": "3b7aaf99-5f35-4c88-a9e1-b80c18d9dabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 4 BOW Logistic Regression Evaluation Metrics:\n",
            "Accuracy: 0.9521\n",
            "Precision:  0.9376\n",
            "Recall:  1.0000\n",
            "F1 Score:  0.9678\n"
          ]
        }
      ],
      "source": [
        "enron4_bow_test_lr = enron4_bow_test.copy()\n",
        "enron4_bow_test_lr['pred_label'] = predict(enron4_bow_test_lr, W_e4_bow)\n",
        "enron4_bow_test_lr['pred_label'] = enron4_bow_test_lr['pred_label'].apply(lambda x: 'spam' if x > 0.5 else 'ham')\n",
        "print(\"Enron 4 BOW Logistic Regression Evaluation Metrics:\")\n",
        "evaluate(enron4_bow_test_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0oQ5nKpLXST",
        "outputId": "96b69853-0182-4e55-a295-6d936e2b3c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 4 Bernoulli Parameter Tuning:\n",
            "t: 1000  ld: 0  lr: 0.0010 validation accuracy: 0.9193\n",
            "t: 1000  ld: 2  lr: 0.0010 validation accuracy: 0.9503\n",
            "t: 1000  ld: 3  lr: 0.0010 validation accuracy: 0.9503\n",
            "t: 1000  ld: 4  lr: 0.0010 validation accuracy: 0.9503\n",
            "t: 1000  ld: 5  lr: 0.0010 validation accuracy: 0.9379\n",
            "t: 1000  ld: 6  lr: 0.0010 validation accuracy: 0.9379\n",
            "t: 1000  ld: 8  lr: 0.0010 validation accuracy: 0.9379\n",
            "t: 1000  ld: 10  lr: 0.0010 validation accuracy: 0.9317\n",
            "t: 1000  ld: 12  lr: 0.0010 validation accuracy: 0.9255\n",
            "t: 1000  ld: 14  lr: 0.0010 validation accuracy: 0.9255\n",
            "t: 1000  ld: 16  lr: 0.0010 validation accuracy: 0.9255\n",
            "t: 1000  ld: 18  lr: 0.0010 validation accuracy: 0.9255\n",
            "t: 1000  ld: 20  lr: 0.0010 validation accuracy: 0.9193\n",
            "Best iterations:1000\n",
            "Best learning rate:0.001\n",
            "Best lambda:2\n"
          ]
        }
      ],
      "source": [
        "print(\"Enron 4 Bernoulli Parameter Tuning:\")\n",
        "e4_bernoulli_params = tuning(enron4_bernoulli_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W_e4_bernoulli = log_regression(enron4_bernoulli_train.to_numpy(), e4_bernoulli_params[0], e4_bernoulli_params[1], e4_bernoulli_params[2])"
      ],
      "metadata": {
        "id": "Bbtr0C5uhcF5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPHJkYpgLeJP",
        "outputId": "237558f0-6559-48a4-dfba-0ee9ead47f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enron 4 Bernoulli Logistic Regression Evaluation Metrics:\n",
            "Accuracy: 0.9558\n",
            "Precision:  0.9465\n",
            "Recall:  0.9949\n",
            "F1 Score:  0.9701\n"
          ]
        }
      ],
      "source": [
        "enron4_bernoulli_test_lr = enron4_bernoulli_test.copy()\n",
        "enron4_bernoulli_test_lr['pred_label'] = predict(enron4_bernoulli_test_lr, W_e4_bernoulli)\n",
        "enron4_bernoulli_test_lr['pred_label'] = enron4_bernoulli_test_lr['pred_label'].apply(lambda x: 'spam' if x > 0.5 else 'ham')\n",
        "print(\"Enron 4 Bernoulli Logistic Regression Evaluation Metrics:\")\n",
        "evaluate(enron4_bernoulli_test_lr)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
